{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAb77yZ9fzMG",
    "outputId": "7db4595a-fada-42f4-eb56-27b11a342569"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers==3.0.0\n",
    "#!pip install tensorflow_datasets\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge_score import scoring\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import shutil\n",
    "import gradio as gr\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pafL7Li0jyXW",
    "outputId": "64b54c44-066e-42b3-dbde-69a4c3d7363a"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFEL_SIZE = 1024\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 3e-5\n",
    "valid_loss_min= np.Inf\n",
    "item_index = 0\n",
    "drivepath=\"Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Storing in Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TuXN5YPfdRI",
    "outputId": "6966c323-403b-4156-dab5-f0321a4309d6"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "cnn_dailymail = tfds.load(name=\"cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhydERzzfmzn"
   },
   "outputs": [],
   "source": [
    "train_tfds = cnn_dailymail['train']\n",
    "val_tfds = cnn_dailymail['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpaHjt5lf08V"
   },
   "outputs": [],
   "source": [
    "train_ds_iter = tfds.as_numpy(train_tfds)\n",
    "val_ds_iter = tfds.as_numpy(val_tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HngjpcHf7zI"
   },
   "outputs": [],
   "source": [
    "def write_data(iter_dataset, name, path=drivepath):\n",
    "    \n",
    "    articles_file = Path(path + name + \"/article\").open(\"w\",encoding=\"utf-8\")\n",
    "    highlights_file = Path(path + name + \"/highlights\").open(\"w\",encoding=\"utf-8\")\n",
    "\n",
    "    for item in iter_dataset:\n",
    "        articles_file.write(item[\"article\"].decode(\"utf-8\") + \"\\n\")\n",
    "        articles_file.flush()\n",
    "        highlights_file.write(item[\"highlights\"].decode(\"utf-8\").replace(\"\\n\", \" \") + \"\\n\")\n",
    "        highlights_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdsZQpIetiqT",
    "outputId": "bdc49c1e-7549-4361-9eb0-5d05d5bf655f"
   },
   "outputs": [],
   "source": [
    "!mkdir train\n",
    "!mkdir test\n",
    "!mkdir val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2iujrn7f_sa"
   },
   "outputs": [],
   "source": [
    "write_data(train_ds_iter, \"train\")\n",
    "write_data(val_ds_iter, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Basic T5 Model\n",
    "### It only contains model architecture. We should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNwRLLUyf_vb"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi9ugZvmf_0w"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, articles, highlights):\n",
    "        self.x = articles\n",
    "        self.y = highlights\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = tokenizer.encode_plus(model.config.prefix + self.transfrom(self.x[index]),truncation=True, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        y = tokenizer.encode(self.transfrom(self.y[index]),truncation=True, max_length=150, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        return x['input_ids'].view(-1), x['attention_mask'].view(-1), y.view(-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transfrom(x):\n",
    "        x = x.lower()\n",
    "        x = re.sub(\"'(.*)'\", r\"\\1\", x)\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(name):\n",
    "    article_path = \"Data/%s/article\" % name\n",
    "    highlights_path = \"Data/%s/highlights\" % name\n",
    "    \n",
    "    articles = [x.rstrip() for x in open(article_path,encoding=\"utf-8\").readlines()]\n",
    "    highlights = [x.rstrip() for x in open(highlights_path,encoding=\"utf-8\").readlines()]\n",
    "    assert len(articles) == len(highlights)\n",
    "    return articles, highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb69vGsVf_3X"
   },
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    articles, highlights = read_files(name)\n",
    "    return MyDataset(articles, highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OL83oB4f_6d"
   },
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\"train\")\n",
    "val_ds = get_dataset(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACKN2JRff_8q"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3YSxePrf_-3"
   },
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "def step(inputs_ids, attention_mask, y):\n",
    "    y_ids = y[:, :-1].contiguous()\n",
    "    lm_labels = y[:, 1:].clone()\n",
    "    lm_labels[y[:, 1:] == pad_token_id] = -100\n",
    "    output = model(inputs_ids, attention_mask=attention_mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "    return output[0] # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGa--wg99Cqc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        torch.save(state, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRrUMBBVgTYt"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "checkpoint_path=\"Data/cp.pt\"\n",
    "best_model_path=\"Data/best.pt\"\n",
    "EPOCHS = 1\n",
    "log_interval = 200\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() \n",
    "    start_time = time.time()\n",
    "    for i, (inputs_ids, attention_mask, y) in enumerate(train_loader):\n",
    "      \n",
    "\n",
    "        if i>item_index:\n",
    "            \n",
    "            inputs_ids = inputs_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = step(inputs_ids, attention_mask, y)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    x, x_mask, y = next(iter(val_loader))\n",
    "                    x = x.to(device)\n",
    "                    x_mask = x_mask.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    v_loss = step(x, x_mask, y)\n",
    "                    v_loss = v_loss.item()\n",
    "\n",
    "\n",
    "                    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "                    print('| epoch {:3d} | [{:5d}/{:5d}] | '\n",
    "                      'ms/batch {:5.2f} | '\n",
    "                      'loss {:5.2f} | val loss {:5.2f}'.format(\n",
    "                        epoch, i, len(train_loader),\n",
    "                        elapsed * 1000 / log_interval,\n",
    "                        loss.item(), v_loss))\n",
    "\n",
    "                    # create checkpoint variable and add important data\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch ,\n",
    "                        'item' : i,\n",
    "                        'valid_loss_min': v_loss,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                      }\n",
    "\n",
    "                    # save checkpoint\n",
    "                    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "                    \n",
    "                    \n",
    "                    ## TODO: save the model if validation loss has decreased\n",
    "                    if v_loss <= valid_loss_min:\n",
    "\n",
    "                        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,v_loss))\n",
    "                        # save checkpoint as best model\n",
    "                        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "                        valid_loss_min = v_loss\n",
    "\n",
    "                start_time = time.time()\n",
    "                val_loss.append(v_loss)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Confidence (Rouge Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXTgey3bDHFn"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RougeScore:\n",
    "    \n",
    "    def __init__(self, score_keys=None)-> None:\n",
    "        super().__init__()\n",
    "        if score_keys is None:  \n",
    "            self.score_keys = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "        \n",
    "        self.scorer = rouge_scorer.RougeScorer(self.score_keys)\n",
    "        self.aggregator = scoring.BootstrapAggregator()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def prepare_summary(summary):\n",
    "            # Make sure the summary is not bytes-type\n",
    "            # Add newlines between sentences so that rougeLsum is computed correctly.\n",
    "            summary = summary.replace(\" . \", \" .\\n\")\n",
    "            return summary\n",
    "    \n",
    "    def __call__(self, target, prediction):\n",
    "        \"\"\"Computes rouge score.''\n",
    "        Args:\n",
    "        targets: string\n",
    "        predictions: string\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.prepare_summary(target)\n",
    "        prediction = self.prepare_summary(prediction)\n",
    "        \n",
    "        self.aggregator.add_scores(self.scorer.score(target=target, prediction=prediction))\n",
    "\n",
    "        return \n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.rouge_list = []\n",
    "\n",
    "    def result(self):\n",
    "        result = self.aggregator.aggregate()\n",
    "        \n",
    "        for key in self.score_keys:\n",
    "            score_text = \"%s = %.2f, 95%% confidence [%.2f, %.2f]\"%(\n",
    "                key,\n",
    "                result[key].mid.fmeasure*100,\n",
    "                result[key].low.fmeasure*100,\n",
    "                result[key].high.fmeasure*100\n",
    "            )\n",
    "            print(score_text)\n",
    "        \n",
    "        return {key: result[key].mid.fmeasure*100 for key in self.score_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds = get_dataset(\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "rouge_score = RougeScore()\n",
    "predictions = []\n",
    "for i, (input_ids, attention_mask) in enumerate(test_loader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    #y = y.to(device)\n",
    "        \n",
    "    summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "    real = [\"Drunk teenage boy climbed into lion enclosure at zoo in west India . Rahul Kumar, 17, ran towards animals \"+\n",
    "            \"shouting 'Today I kill a lion!' Fortunately he fell into a moat before reaching lions and was rescued .\"]\n",
    "    for pred_sent, real_sent in zip(pred, real):\n",
    "        rouge_score(pred_sent, real_sent)\n",
    "        predictions.append(str(\"pred sentence: \" + pred_sent + \"\\n\\n real sentence: \" + real_sent))\n",
    "    if i > 40:\n",
    "        break\n",
    "    \n",
    "rouge_score.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19FUWvK_DNmD"
   },
   "outputs": [],
   "source": [
    "rouge_score = RougeScore()\n",
    "predictions = []\n",
    "for i, (input_ids, attention_mask, y) in enumerate(test_loader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "    real = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in y]\n",
    "    for pred_sent, real_sent in zip(pred, real):\n",
    "        rouge_score(pred_sent, real_sent)\n",
    "        predictions.append(str(\"pred sentence: \" + pred_sent + \"\\n\\n real sentence: \" + real_sent))\n",
    "    if i > 40:\n",
    "        break\n",
    "    \n",
    "rouge_score.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZesOfkGCp7J"
   },
   "source": [
    "# **Loading the saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXcGyP3NpJZZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    item_index= checkpoint['item']\n",
    "    return model,item_index, optimizer, checkpoint['epoch'], valid_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cg4w8L7kOgTq",
    "outputId": "4abd8d95-2c3c-4448-ed34-98d2cb5547cc"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "task_specific_params = model.config.task_specific_params\n",
    "if task_specific_params is not None:\n",
    "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
    "best_model_path=\"Data/cp.pt\"\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay=0.0001)\n",
    "model,item_index, optimizer, start_epoch, valid_loss_min = load_ckp(best_model_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Model Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXRKMyJoqkRV",
    "outputId": "2f29d38c-45d5-4c5c-83ad-b7c453e3e4a7"
   },
   "outputs": [],
   "source": [
    "print(\"model = \", model)\n",
    "print(\"optimizer = \", optimizer)\n",
    "print(\"Item_index = \",item_index)\n",
    "print(\"start_epoch = \", start_epoch)\n",
    "print(\"valid_loss_min = \", valid_loss_min)\n",
    "print(\"valid_loss_min = {:.6f}\".format(valid_loss_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vqHnfci7ypA",
    "outputId": "acee3758-64dc-4797-d2bf-899fb4ab0326"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_files(name):\n",
    "    article_path = \"Data/%s/article\" % name\n",
    "    articles = [x.rstrip() for x in open(article_path).readlines()]\n",
    "    return articles\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, articles):\n",
    "        self.x = articles\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = tokenizer.encode_plus(model.config.prefix + self.transfrom(self.x[index]),truncation=True, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        #y = tokenizer.encode(self.transfrom(self.y[index]),truncation=True, max_length=150, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "        return x['input_ids'].view(-1), x['attention_mask'].view(-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transfrom(x):\n",
    "        x = x.lower()\n",
    "        x = re.sub(\"'(.*)'\", r\"\\1\", x)\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "def get_dataset(name):\n",
    "    articles= read_files(name)\n",
    "    return MyDataset(articles) \n",
    "def predict(blog):\n",
    "    test_str=blog\n",
    "    articles_file = Path(drivepath+\"test/article\").open(\"w\")\n",
    "    articles_file.write(test_str)\n",
    "    articles_file.close()\n",
    "    test_ds = get_dataset(\"test\")\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE) \n",
    "    for i, (input_ids, attention_mask) in enumerate(test_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        model.eval() \n",
    "        summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
    "        return pred[0]\n",
    "        #print(\"Real sentence : \", test_str)\n",
    "        #print(\"predicted text: \",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset(\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "for i, (input_ids, attention_mask) in enumerate(test_loader):\n",
    "    print(input_ids)\n",
    "    print(attention_mask)\n",
    "    summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    for g in summaries:\n",
    "        print(g)\n",
    "        print(tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface for Input Text and Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "sample=\"A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at the Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'. Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!' A zoo spokesman said: 'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. 'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. 'We then handed him over to the police.' Brave fool: Fortunately, Mr Kumar  fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it. 'I was drunk and thought I'd stand a good chance.' A police spokesman said: 'He has been cautioned and will be sent for psychiatric evaluation. 'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.\"\n",
    "gr.Interface(fn=predict,inputs= [gr.inputs.Textbox(lines=1000000 ,label=\"Enter Text to Summarise\",default=sample, placeholder=\"Start here...\")],outputs=[gr.outputs.Textbox( type=\"auto\", label=None)]).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Text summarization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
